import os
import json
import cv2
import numpy as np
import pytesseract
from PIL import Image
from tqdm import tqdm
import config


def detect_tiff_type(image_path):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø TIFF —Ñ–∞–π–ª–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    from PIL import Image

    with Image.open(image_path) as img:
        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        compression = img.info.get("compression", "unknown")
        width, height = img.size

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        if "group" in str(compression).lower() or compression == "group4":
            return "compressed_document"  # –°–∂–∞—Ç—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        elif compression == "raw" or "none" in str(compression).lower():
            return "uncompressed_scan"  # –ù–µ—Å–∂–∞—Ç—ã–µ —Å–∫–∞–Ω—ã
        else:
            return "standard_tiff"  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ TIFF


def preprocess_image(image_path):
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ OCR"""

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø TIFF –¥–ª—è –≤—ã–±–æ—Ä–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    tiff_type = detect_tiff_type(image_path)
    print(f"–¢–∏–ø TIFF: {tiff_type}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞–∫ grayscale —Å—Ä–∞–∑—É –¥–ª—è TIFF —Ñ–∞–π–ª–æ–≤
    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if image is None:
        raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_path}")

    print(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {image.shape}, —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {np.mean(image):.1f}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω—É–∂–Ω–∞ –ª–∏ –∏–Ω–≤–µ—Ä—Å–∏—è (–¥–ª—è WhiteIsZero TIFF)
    if np.mean(image) < 128:
        image = cv2.bitwise_not(image)
        print("–ü—Ä–∏–º–µ–Ω–µ–Ω–∞ –∏–Ω–≤–µ—Ä—Å–∏—è")

    # –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞
    height, width = image.shape
    total_pixels = height * width

    print(f"–í—Å–µ–≥–æ –ø–∏–∫—Å–µ–ª–µ–π: {total_pixels:,}")

    # –¶–µ–ª–µ–≤–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –¥–ª—è OCR - –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Å–∫—Ä–∏–Ω—à–æ—Ç
    target_pixels = 2_000_000  # ~1414x1414 –∏–ª–∏ 1920x1040

    if total_pixels > target_pixels * 2:  # –û—á–µ–Ω—å –±–æ–ª—å—à–∏–µ - –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ —É–º–µ–Ω—å—à–∞–µ–º
        scale_factor = 0.4  # –°–∏–ª—å–Ω–æ —É–º–µ–Ω—å—à–∞–µ–º
        interpolation = cv2.INTER_AREA
        print("–û—á–µ–Ω—å –±–æ–ª—å—à–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ - —Å–∏–ª—å–Ω–æ —É–º–µ–Ω—å—à–∞–µ–º")
    elif total_pixels > target_pixels:  # –ë–æ–ª—å—à–∏–µ - —É–º–µ–Ω—å—à–∞–µ–º
        scale_factor = 0.6  # –£–º–µ—Ä–µ–Ω–Ω–æ —É–º–µ–Ω—å—à–∞–µ–º
        interpolation = cv2.INTER_AREA
        print("–ë–æ–ª—å—à–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ - —É–º–µ–Ω—å—à–∞–µ–º")
    elif total_pixels < target_pixels // 4:  # –û—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–µ - —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º
        scale_factor = 2.0  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –º–∞–ª–µ–Ω—å–∫–∏–µ
        interpolation = cv2.INTER_CUBIC
        print("–ú–∞–ª–µ–Ω—å–∫–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ - —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º")
    else:  # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä - –Ω–µ —Ç—Ä–æ–≥–∞–µ–º
        scale_factor = 1.0
        interpolation = cv2.INTER_LANCZOS4
        print("–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä - –Ω–µ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º")

    if scale_factor != 1.0:
        image = cv2.resize(image, (int(width * scale_factor), int(height * scale_factor)), interpolation=interpolation)

    print(f"–§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {image.shape}")

    # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
    # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ - –∏–º–∏—Ç–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞
    # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏

    # –û—á–µ–Ω—å –ª–µ–≥–∫–∏–π –¥–µ–Ω–æ–π–∑–∏–Ω–≥ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –æ—á–µ–Ω—å –∑–∞—à—É–º–ª–µ–Ω–Ω–æ–µ
    if total_pixels > 4_000_000:  # –¢–æ–ª—å–∫–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
        denoised = cv2.fastNlMeansDenoising(image, h=5)  # –°–ª–∞–±—ã–π –¥–µ–Ω–æ–π–∑–∏–Ω–≥
    else:
        denoised = image  # –ú–∞–ª–µ–Ω—å–∫–∏–µ —Ñ–∞–π–ª—ã –Ω–µ —Ç—Ä–æ–≥–∞–µ–º

    # –ü—Ä–æ—Å—Ç–∞—è OTSU –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è - –∫–∞–∫ –¥–µ–ª–∞—é—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–∫–∞–Ω–µ—Ä—ã
    _, final_thresh = cv2.threshold(denoised, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –ø—Ä–æ—Å—Ç–∞—è OTSU –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è (—Å–∫—Ä–∏–Ω—à–æ—Ç-—Å—Ç–∏–ª—å)")

    return final_thresh


def extract_text_from_image(image_path):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é Tesseract OCR"""

    try:
        processed_image = preprocess_image(image_path)

        pil_image = Image.fromarray(processed_image)

        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Å—Ç–∞—Ä—ã—Ö —Ä—É—Å—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        # PSM 4 - –æ–¥–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
        # –î–æ–±–∞–≤–ª—è–µ–º whitelist –¥–ª—è –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è tesseract –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
        tiff_type = detect_tiff_type(image_path)

        if tiff_type == "uncompressed_scan":
            # –î–ª—è –Ω–µ—Å–∂–∞—Ç—ã—Ö —Å–∫–∞–Ω–æ–≤ - –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
            custom_config = f"--oem 3 --psm 4 -l {config.TESSERACT_LANGUAGES} -c preserve_interword_spaces=1"
        else:
            # –î–ª—è —Å–∂–∞—Ç—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
            custom_config = f"--oem 3 --psm 4 -l {config.TESSERACT_LANGUAGES} -c preserve_interword_spaces=1"

        text = pytesseract.image_to_string(pil_image, config=custom_config)

        return text.strip()

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {image_path}: {str(e)}")
        return ""


def process_tiff_file(file_path, output_dir, file_metadata=None):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ TIFF —Ñ–∞–π–ª–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ JSON"""

    filename = os.path.basename(file_path)
    file_id = os.path.splitext(filename)[0]

    print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ñ–∞–π–ª: {filename}")

    extracted_text = extract_text_from_image(file_path)

    if not extracted_text:
        print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –¢–µ–∫—Å—Ç –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω –∏–∑ {filename}")

    result = {"file_id": file_id, "filename": filename, "text": extracted_text, "text_length": len(extracted_text)}

    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–± –∞—Ä—Ö–∏–≤–µ –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã
    if file_metadata:
        result.update(
            {
                "unique_id": file_metadata.get("unique_id", file_id),
                "original_name": file_metadata.get("original_name", filename),
                "original_path": file_metadata.get("original_path", ""),
                "archive_source": file_metadata.get("archive_source", ""),
                "archive_id": file_metadata.get("archive_id", ""),
            }
        )

    output_path = os.path.join(output_dir, f"{file_id}.json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
    return result


def process_all_files():
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö TIFF —Ñ–∞–π–ª–æ–≤ –∏–∑ –ø–∞–ø–∫–∏ tiff_reports"""

    source_dir = config.DATA_PATHS["tiff_reports"]
    output_dir = config.DATA_PATHS["extracted_text"]

    os.makedirs(output_dir, exist_ok=True)

    tiff_files = [f for f in os.listdir(source_dir) if f.lower().endswith((".tif", ".tiff"))]

    if not tiff_files:
        print("TIFF —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –ø–∞–ø–∫–µ tiff_reports!")
        return

    print(f"–ù–∞–π–¥–µ–Ω–æ {len(tiff_files)} TIFF —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

    results = []
    failed_files = []

    for filename in tqdm(tiff_files, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ TIFF —Ñ–∞–π–ª–æ–≤"):
        file_path = os.path.join(source_dir, filename)
        try:
            result = process_tiff_file(file_path, output_dir)
            results.append(result)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {filename}: {str(e)}")
            failed_files.append(filename)

    summary = {
        "total_files": len(tiff_files),
        "processed_files": len(results),
        "failed_files": len(failed_files),
        "successful_extractions": len([r for r in results if r["text_length"] > 0]),
        "average_text_length": sum(r["text_length"] for r in results) / len(results) if results else 0,
    }

    print(f"\nüèÅ –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–í–û–î–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò:")
    print(f"üìÅ –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {summary['total_files']}")
    print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {summary['processed_files']}")
    print(f"‚ùå –û—à–∏–±–æ–∫: {summary['failed_files']}")
    print(f"üìù –£—Å–ø–µ—à–Ω—ã—Ö –∏–∑–≤–ª–µ—á–µ–Ω–∏–π: {summary['successful_extractions']}")
    print(f"üìä –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {summary['average_text_length']:.0f} —Å–∏–º–≤–æ–ª–æ–≤")

    if failed_files:
        print(f"\n‚ùå –§–∞–π–ª—ã —Å –æ—à–∏–±–∫–∞–º–∏: {', '.join(failed_files[:10])}{'...' if len(failed_files) > 10 else ''}")

    return results


def process_test_files():
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤"""

    test_dir = config.DATA_PATHS["test_files"]
    output_dir = config.DATA_PATHS["extracted_text"]

    os.makedirs(output_dir, exist_ok=True)

    tiff_files = [f for f in os.listdir(test_dir) if f.lower().endswith((".tif", ".tiff"))]

    if not tiff_files:
        print("–¢–µ—Å—Ç–æ–≤—ã–µ TIFF —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
        return

    print(f"–ù–∞–π–¥–µ–Ω–æ {len(tiff_files)} —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤")

    results = []
    for filename in tqdm(tiff_files, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤"):
        file_path = os.path.join(test_dir, filename)
        result = process_tiff_file(file_path, output_dir)
        results.append(result)

    summary = {
        "total_files": len(results),
        "successful_extractions": len([r for r in results if r["text_length"] > 0]),
        "average_text_length": sum(r["text_length"] for r in results) / len(results) if results else 0,
    }

    print(f"\n–°–≤–æ–¥–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
    print(f"–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {summary['total_files']}")
    print(f"–£—Å–ø–µ—à–Ω—ã—Ö –∏–∑–≤–ª–µ—á–µ–Ω–∏–π: {summary['successful_extractions']}")
    print(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {summary['average_text_length']:.0f} —Å–∏–º–≤–æ–ª–æ–≤")

    return results


def process_extracted_files(batch_size=10, resume_from=None):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –∏–∑ –∞—Ä—Ö–∏–≤–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π checkpoint'–æ–≤"""

    from archive_processor import ArchiveProcessor

    processor = ArchiveProcessor()
    metadata = processor.load_metadata()

    if not metadata or not metadata.get("archives"):
        print("–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∞—Ä—Ö–∏–≤–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—Ä–∞–±–æ—Ç–∫—É –∞—Ä—Ö–∏–≤–æ–≤.")
        return []

    all_files = processor.get_all_extracted_files()
    if not all_files:
        print("–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        return []

    output_dir = config.DATA_PATHS["extracted_text"]
    os.makedirs(output_dir, exist_ok=True)

    # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ resume - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
    processed_files = set()
    if resume_from:
        existing_files = [f for f in os.listdir(output_dir) if f.endswith(".json")]
        processed_files = {os.path.splitext(f)[0] for f in existing_files}
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(processed_files)} —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")

    total_files = len(all_files)
    results = []
    failed_files = []

    print(f"–ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É {total_files} —Ñ–∞–π–ª–æ–≤ –∏–∑ –∞—Ä—Ö–∏–≤–æ–≤...")
    print(f"–†–∞–∑–º–µ—Ä batch: {batch_size}")

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞–º–∏ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é
    for i in range(0, total_files, batch_size):
        batch = all_files[i : i + batch_size]
        batch_num = (i // batch_size) + 1
        total_batches = (total_files + batch_size - 1) // batch_size

        print(f"\n=== Batch {batch_num}/{total_batches} ({len(batch)} —Ñ–∞–π–ª–æ–≤) ===")

        for file_info in tqdm(batch, desc=f"Batch {batch_num}"):
            unique_id = file_info["unique_id"]

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
            if unique_id in processed_files:
                print(f"–ü—Ä–æ–ø—É—Å–∫–∞—é {unique_id} (—É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω)")
                continue

            file_path = file_info["extracted_path"]

            if not os.path.exists(file_path):
                print(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
                failed_files.append(unique_id)
                continue

            try:
                # –ü–µ—Ä–µ–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∞—Ä—Ö–∏–≤–∞
                archive_metadata = {
                    "unique_id": unique_id,
                    "original_name": file_info["original_name"],
                    "original_path": file_info["original_path"],
                    "archive_source": file_info["archive_source"],
                    "archive_id": file_info.get("archive_id", ""),
                }

                result = process_tiff_file(file_path, output_dir, archive_metadata)
                results.append(result)
                processed_files.add(unique_id)

            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {unique_id}: {str(e)}")
                failed_files.append(unique_id)

        # Checkpoint –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ batch
        print(f"Batch {batch_num} –∑–∞–≤–µ—Ä—à–µ–Ω. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(results)}, –æ—à–∏–±–æ–∫: {len(failed_files)}")

    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞
    summary = {
        "total_files": total_files,
        "processed_files": len(results),
        "failed_files": len(failed_files),
        "successful_extractions": len([r for r in results if r["text_length"] > 0]),
        "average_text_length": sum(r["text_length"] for r in results) / len(results) if results else 0,
    }

    print(f"\nüèÅ –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–í–û–î–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò –ê–†–•–ò–í–û–í:")
    print(f"üìÅ –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {summary['total_files']}")
    print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {summary['processed_files']}")
    print(f"‚ùå –û—à–∏–±–æ–∫: {summary['failed_files']}")
    print(f"üìù –£—Å–ø–µ—à–Ω—ã—Ö –∏–∑–≤–ª–µ—á–µ–Ω–∏–π: {summary['successful_extractions']}")
    print(f"üìä –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {summary['average_text_length']:.0f} —Å–∏–º–≤–æ–ª–æ–≤")

    if failed_files:
        print(f"\n‚ùå –§–∞–π–ª—ã —Å –æ—à–∏–±–∫–∞–º–∏: {', '.join(failed_files[:10])}{'...' if len(failed_files) > 10 else ''}")

    return results


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "--all":
        print("üöÄ –ó–ê–ü–£–°–ö –û–ë–†–ê–ë–û–¢–ö–ò –í–°–ï–• –§–ê–ô–õ–û–í!")
        results = process_all_files()
    elif len(sys.argv) > 1 and sys.argv[1] == "--archives":
        print("üóÇÔ∏è –ó–ê–ü–£–°–ö –û–ë–†–ê–ë–û–¢–ö–ò –§–ê–ô–õ–û–í –ò–ó –ê–†–•–ò–í–û–í!")
        batch_size = int(sys.argv[2]) if len(sys.argv) > 2 else 10
        resume = True if len(sys.argv) > 3 and sys.argv[3] == "--resume" else False
        results = process_extracted_files(batch_size=batch_size, resume_from=resume)
    else:
        print("üß™ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤")
        print("–î–ª—è –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤: --all")
        print("–î–ª—è –∞—Ä—Ö–∏–≤–æ–≤: --archives [batch_size] [--resume]")
        results = process_test_files()
