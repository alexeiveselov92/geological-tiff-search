import os
import pickle
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Tuple
import config

class GeologicalSearchEngine:
    """–ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self, index_path: str = None):
        if index_path is None:
            index_path = os.path.join(config.DATA_PATHS["embeddings"], "search_index.pkl")
        
        self.index_path = index_path
        self.index_data = None
        self.model = None
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if os.path.exists(self.index_path):
            self.load_index()
    
    def build_index(self):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –∏–∑ embeddings"""
        from embeddings_creator import EmbeddingsCreator
        
        print("üîç –°—Ç—Ä–æ—é –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —É–∂–µ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ embeddings
        embeddings_dir = config.DATA_PATHS["embeddings"]
        if not os.path.exists(embeddings_dir):
            raise FileNotFoundError(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å embeddings –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {embeddings_dir}")
        
        embedding_files = [f for f in os.listdir(embeddings_dir) if f.endswith('_embeddings.json')]
        if not embedding_files:
            raise FileNotFoundError("–ù–µ –Ω–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã —Å embeddings. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–æ–∑–¥–∞–Ω–∏–µ embeddings.")
        
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(embedding_files)} —Ñ–∞–π–ª–æ–≤ —Å embeddings")
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å
        creator = EmbeddingsCreator()
        index_path = creator.create_search_index(embeddings_dir)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å
        self.load_index()
        
        return index_path
    
    def load_index(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞"""
        if not os.path.exists(self.index_path):
            raise FileNotFoundError(f"–ü–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.index_path}")
        
        print(f"–ó–∞–≥—Ä—É–∂–∞—é –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å: {self.index_path}")
        
        with open(self.index_path, 'rb') as f:
            self.index_data = pickle.load(f)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
        model_name = self.index_data["model_name"]
        print(f"–ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å: {model_name}")
        self.model = SentenceTransformer(model_name)
        
        print(f"–ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω:")
        print(f"- –ú–æ–¥–µ–ª—å: {self.index_data['model_name']}")
        print(f"- –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {self.index_data['total_chunks']}")
        print(f"- –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {self.index_data['embedding_dim']}")
    
    def vectorize_query(self, query: str) -> np.ndarray:
        """–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        query_vector = self.model.encode([query])[0]
        return query_vector
    
    def search(self, query: str, top_k: int = 5, min_similarity: float = 0.0) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
            min_similarity: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ —Å –æ—Ü–µ–Ω–∫–∞–º–∏ —Å—Ö–æ–¥—Å—Ç–≤–∞
        """
        if self.index_data is None:
            raise ValueError("–ò–Ω–¥–µ–∫—Å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω")
        
        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞
        query_vector = self.vectorize_query(query)
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
        similarities = cosine_similarity(
            query_vector.reshape(1, -1), 
            self.index_data["embeddings"]
        )[0]
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞
        ranked_indices = np.argsort(similarities)[::-1]
        
        results = []
        for i, idx in enumerate(ranked_indices):
            if i >= top_k:
                break
            
            similarity = similarities[idx]
            if similarity < min_similarity:
                break
            
            chunk = self.index_data["chunks"][idx].copy()
            chunk["similarity"] = float(similarity)
            chunk["rank"] = i + 1
            
            results.append(chunk)
        
        return results
    
    def search_with_details(self, query: str, top_k: int = 5) -> Dict:
        """
        –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ–∏—Å–∫–∞ –∏ –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        results = self.search(query, top_k)
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ —Ñ–∞–π–ª–∞–º
        files_found = {}
        for result in results:
            file_id = result["file_id"]
            if file_id not in files_found:
                files_found[file_id] = []
            files_found[file_id].append(result)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        avg_similarity = np.mean([r["similarity"] for r in results]) if results else 0
        max_similarity = max([r["similarity"] for r in results]) if results else 0
        
        return {
            "query": query,
            "total_results": len(results),
            "results": results,
            "files_found": files_found,
            "stats": {
                "average_similarity": float(avg_similarity),
                "max_similarity": float(max_similarity),
                "files_count": len(files_found)
            }
        }
    
    def get_chunk_context(self, chunk_id: str, context_size: int = 1) -> List[Dict]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤–æ–∫—Ä—É–≥ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —á–∞–Ω–∫–∞
        
        Args:
            chunk_id: ID —á–∞–Ω–∫–∞
            context_size: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Å–µ–¥–Ω–∏—Ö —á–∞–Ω–∫–æ–≤ —Å –∫–∞–∂–¥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã
        
        Returns:
            –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –≤–∫–ª—é—á–∞—è –∫–æ–Ω—Ç–µ–∫—Å—Ç
        """
        # –ù–∞—Ö–æ–¥–∏–º —Ü–µ–ª–µ–≤–æ–π —á–∞–Ω–∫
        target_chunk = None
        target_idx = None
        
        for i, chunk in enumerate(self.index_data["chunks"]):
            if chunk["chunk_id"] == chunk_id:
                target_chunk = chunk
                target_idx = i
                break
        
        if target_chunk is None:
            return []
        
        file_id = target_chunk["file_id"]
        chunk_index = target_chunk["chunk_index"]
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —á–∞–Ω–∫–∏ –∏–∑ —Ç–æ–≥–æ –∂–µ —Ñ–∞–π–ª–∞
        file_chunks = []
        for chunk in self.index_data["chunks"]:
            if chunk["file_id"] == file_id:
                file_chunks.append(chunk)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏–Ω–¥–µ–∫—Å—É —á–∞–Ω–∫–∞
        file_chunks.sort(key=lambda x: x["chunk_index"])
        
        # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏—é —Ü–µ–ª–µ–≤–æ–≥–æ —á–∞–Ω–∫–∞ –≤ —Ñ–∞–π–ª–µ
        target_pos = None
        for i, chunk in enumerate(file_chunks):
            if chunk["chunk_id"] == chunk_id:
                target_pos = i
                break
        
        if target_pos is None:
            return [target_chunk]
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        start = max(0, target_pos - context_size)
        end = min(len(file_chunks), target_pos + context_size + 1)
        
        context_chunks = file_chunks[start:end]
        
        # –û—Ç–º–µ—á–∞–µ–º —Ü–µ–ª–µ–≤–æ–π —á–∞–Ω–∫
        for chunk in context_chunks:
            chunk["is_target"] = (chunk["chunk_id"] == chunk_id)
        
        return context_chunks

def test_search_engine():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã"""
    try:
        search_engine = GeologicalSearchEngine()
        
        # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
        test_queries = [
            "–º–µ—Å—Ç–æ—Ä–æ–∂–¥–µ–Ω–∏–µ",
            "–ø–µ—Å–æ–∫ –≥—Ä–∞–≤–∏–π",
            "–ë–æ—Ä–∏—Å–æ–≤–æ",
            "–≥–µ–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–≤–µ–¥–∫–∞",
            "–ü—Ä–æ—Ç–≤–∞",
            "1959"
        ]
        
        print("\n=== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–û–ò–°–ö–û–í–û–ô –°–ò–°–¢–ï–ú–´ ===\n")
        
        for query in test_queries:
            print(f"\n–ó–∞–ø—Ä–æ—Å: '{query}'")
            print("-" * 50)
            
            search_results = search_engine.search_with_details(query, top_k=3)
            
            print(f"–ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {search_results['total_results']}")
            print(f"–§–∞–π–ª–æ–≤ –∑–∞—Ç—Ä–æ–Ω—É—Ç–æ: {search_results['stats']['files_count']}")
            print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: {search_results['stats']['max_similarity']:.3f}")
            
            for i, result in enumerate(search_results['results'], 1):
                print(f"\n{i}. –§–∞–π–ª: {result['filename']}")
                print(f"   –ß–∞–Ω–∫: {result['chunk_id']}")
                print(f"   –°—Ö–æ–¥—Å—Ç–≤–æ: {result['similarity']:.3f}")
                print(f"   –¢–µ–∫—Å—Ç: {result['text'][:200]}...")
        
        return search_engine
    
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        return None

# –ê–ª–∏–∞—Å –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
SearchEngine = GeologicalSearchEngine

if __name__ == "__main__":
    test_search_engine()